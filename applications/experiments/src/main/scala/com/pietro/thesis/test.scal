package com.pietro.thesis

import org.apache.kafka.clients.producer.{KafkaProducer, ProducerConfig, ProducerRecord}

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.streaming._
import org.apache.spark.streaming.dstream._
import org.apache.spark.streaming.kafka010._
import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent
import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe
import org.apache.kafka.common.TopicPartition
import org.apache.kafka.clients.producer._
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.Encoders

import java.util.Properties
import java.io.FileInputStream

import org.apache.log4j._
import scala.concurrent.duration._

object ClusterLocalMGSummaryTest {
  def main(args: Array[String]) {    

    val prop = new Properties()
    prop.load(new FileInputStream("app.properties"))
    val clusterId = prop.getProperty("cluster.id")
    
    /* Input Kafka configuration (which topic read as input and from which server)
     * Two input topics: cluster input data and cluster output data */
    val clusterInputTopic = prop.getProperty("cluster.input.topic")
    val clusterInputBrokers = prop.getProperty("cluster.input.brokers")

    val clusterSummaryTopic = prop.getProperty("cluster.output.topic")
    val clusterSummaryBrokers = prop.getProperty("cluster.output.brokers")

    /* Output Kafka configuration (which topic to write on and from which server) */
    val outputTopic = prop.getProperty("test.cluster.output.topic")
    val outputBrokers = prop.getProperty("test.cluster.output.brokers")

    /* Sliding window parameters */
    val windowLength = Minutes(prop.getProperty("cluster.windowlength.minutes").toInt)
    val slideInterval = Minutes(prop.getProperty("cluster.windowslide.minutes").toInt)
    
    import org.apache.spark.sql._
    import org.apache.spark.sql.SparkSession
    import org.apache.spark.sql.Dataset
    import org.apache.spark.sql.Row
    import org.apache.spark.sql.streaming.StreamingQuery
    import org.apache.spark.sql.types._
    import org.apache.spark.sql.functions._
    import org.apache.spark.sql.functions.lit

    val spark = SparkSession
        .builder()
        .appName("TestSSComputeClusterLocal")
        .getOrCreate()

    import spark.implicits._

    /* Compute exact word count from input cluster data */
    val schema = new StructType()
        .add("created_at", StringType)
        .add("value", StringType)
        .add("producer_id", StringType)

    val records = spark
        .readStream
        .format("kafka")
        .option("kafka.bootstrap.servers", clusterInputBrokers)
        .option("subscribe", clusterInputTopic)
        .option("startingOffsets", "earliest")
        .load()
	.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)") // cast key and value as string to be read in the following

    /* Extract 'value' according to provided schema and adding column with timestamp */
    val parsedRecords = records
	.select( from_json($"value", schema) as "parsed" )
        .select( unix_timestamp($"parsed.created_at", "yyyy-MM-dd'T'HH:mm:ss").cast("timestamp").alias("timestamp"), $"parsed.*" )

    import org.apache.spark.sql.expressions.UserDefinedAggregateFunction
    spark.udf.register("myAverage", CreateMGSummary)
    lazy val summary: UserDefinedAggregateFunction = CreateMGSummary

    /* Windowed computation: count all elements and sum them up */
    val trueWordCount = parsedRecords
//        .withWatermark("timestamp", "60 minutes")
	.groupBy(window($"timestamp", "10 minutes", "1 minute") as "window")
	.agg(summary('value).as("exactFrequencies"))
	.filter($"window.end".as("timestamp") < current_timestamp()) // filter window starting now and ending 'windowLength' in the future
	.orderBy($"window".desc)
	.select($"window", $"exactFrequencies")

    /* Show schema after computation */
    trueWordCount.printSchema()
    
    import org.apache.spark.sql.streaming.{OutputMode, Trigger, ProcessingTime}

    /* Print on console */
    val wordCountToConsole = trueWordCount
	.writeStream
   	.format("json")
	.option("path", "/home/pietro/thesis03/out.json")
	.option("checkpointLocation", "json.chkpnt")
//	.queryName("word_count")
//	.option("truncate", "true")
	.outputMode("update")
//	.trigger(ProcessingTime(60.seconds))
    	.start()

//    spark.sql("select * from word_count").show()

//    wordCountToConsole.awaitTermination

/*
    /* Get summaries from MGSummaries topic */
    val summarySchema = new StructType()
        .add("window", new StructType().add("start", TimestampType).add("end", TimestampType))
        .add("MG summary", StringType)
        .add("clusterId", StringType)

    val recordsSummaries = spark
        .readStream
        .format("kafka")
        .option("kafka.bootstrap.servers", clusterSummaryBrokers)
        .option("subscribe", clusterSummaryTopic)
        .option("startingOffsets", "earliest")
        .load()
        .selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)") // cast key and value as string to be read in the following

    val summaries = recordsSummaries
        .select( from_json($"value", summarySchema) as "parsed" )
        .select( $"parsed.window", $"parsed.MG summary", $"parsed.clusterId" )
	.withColumn("windowStart", $"window.start")
//	.withWatermark("windowStart", "60 minutes")

    summaries.printSchema()

    val summariesTable = summaries
	.writeStream
	.format("console")
//	.queryName("summariesTbl")
	.outputMode("update")
	.trigger(ProcessingTime(10.seconds))

//    summariesTable.createOrReplaceTempView("sumTable")

//    spark.sql("select * from sumTable").show()
    spark.sql("select * from wordCount").show()

    val table = spark.table("wordCount")

    /* Print on console */
    val toConsole = summaries
	.join( table, "window")
        .writeStream
        .format("console")
        .option("truncate","false")
        .outputMode("update")
        .trigger(ProcessingTime(10.seconds))
        .start()
*/
    /* Compare results */


/*
    /* Produce on Kafka topic */
    val toKafka = windowedRecords
	.selectExpr("CAST(window AS STRING) AS key", "to_json(struct(*)) AS value")
        .writeStream
	.outputMode("complete")
	.trigger(ProcessingTime(20.seconds))
        .format("kafka")
        .option("kafka.bootstrap.servers", outputBrokers)
        .option("topic", outputTopic)
        .option("checkpointLocation", "ccl-ss") // ComputeClusterLocal-StructuredStreaming
        .start()
*/    

//    toConsole.awaitTermination
//    toKafka.awaitTermination

  }
}

  import org.apache.spark.sql.{Row, SparkSession}
  import org.apache.spark.sql.expressions.MutableAggregationBuffer
  import org.apache.spark.sql.expressions.UserDefinedAggregateFunction
  import org.apache.spark.sql.types._
  import org.apache.spark.annotation.InterfaceStability
  import org.apache.spark.sql.{Column, Row}
  import org.apache.spark.sql.catalyst.expressions.aggregate.{AggregateExpression, Complete}
  import org.apache.spark.sql.execution.aggregate.ScalaUDAF

  object CreateMGSummary extends UserDefinedAggregateFunction {

     // Data types of input arguments of this aggregate function
     def inputSchema: StructType = new StructType().add("element", StringType)

     // Data types of values in the aggregation buffer
     def bufferSchema: StructType = new StructType().add("buffer1", MapType(StringType, LongType))
   
     // The data type of the returned value
     def dataType: DataType = StringType
   
     // Whether this function always returns the same output on the identical input
     def deterministic: Boolean = true

     // Initializes the given aggregation buffer. The buffer itself is a `Row` that in addition to
     // standard methods like retrieving a value at an index (e.g., get(), getBoolean()), provides
     // the opportunity to update its values. Note that arrays and maps inside the buffer are still
     // immutable.
     def initialize(buffer: MutableAggregationBuffer): Unit = {
       buffer(0) = Map()
     }
   
     // Updates the given aggregation buffer `buffer` with new input data from `input`
     def update(buffer: MutableAggregationBuffer, input: Row): Unit = {
       if (!input.isNullAt(0)) {
   	   var map = buffer.getAs[Map[String, Long]](0)
	   val keyToAdd = input.getString(0)
   	   
	   if(map.contains(keyToAdd)){
              /* The elements is already contained: increment the count */
	      val value = map.get(keyToAdd).get + 1
	      map = map + (keyToAdd -> value)
	   } else{
              /* There is space to add the element in the summary */
	      map = map + (keyToAdd -> 1)
	   } 

	   buffer(0) = map
       }
     }
      
     // Merges two aggregation buffers and stores the updated buffer values back to `buffer1`
     def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {

	var list = List[(String, Long)]()
	if (!buffer2.isNullAt(0) && !buffer1.isNullAt(0)){
   	   list = buffer1.getAs[Map[String, Long]](0).toList ::: buffer2.getAs[Map[String, Long]](0).toList
	}	

	if(list.length > 0){
	   def sum(xs: List[Long]): Long = {
  	     xs match {
	       case x :: tail => x + sum(tail) // if there is an element, add it to the sum of the tail
	       case Nil => 0 // if there are no elements, then the sum is 0
	     }
	   }

	   var sortedList = list.groupBy(_._1)
	      .map { case (k,v) => (k, sum(v.map(_._2))) }
	      .toList
	      .sortWith(_._2 > _._2)

	   buffer1(0) = sortedList.toMap

	} 
     }

     // Calculates the final result
     def evaluate(buffer: Row): String = {
	var i = 0
	var summary = ""
	val map = buffer.getAs[Map[String, Long]](0).toSeq.sortWith(_._2 > _._2)
	for( (k,v) <- map) {
	   if( i == 0 ) {
	      summary = summary + s"""{"value":"${k}","frequency":${v}}"""
	      i = 1
	   } else
              summary = summary + s""",{"value":"${k}","frequency":${v}}"""	   

	}
	   
	s"""{"exactSize":${map.size},"exactSummary":[${summary}]}"""
	
     }
   }


