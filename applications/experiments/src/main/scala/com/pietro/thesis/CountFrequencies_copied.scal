package com.pietro.thesis

// scalastyle:off println
import scala.io.Source
import scala.util.Random
import scala.util._

import org.apache.kafka.clients.producer.{KafkaProducer, ProducerConfig, ProducerRecord}

import org.apache.spark.{SparkConf, SparkEnv, SparkContext}
import org.apache.spark.streaming._
import org.apache.spark.streaming.dstream._
import org.apache.spark.rdd.RDD
import java.util.Properties
import java.io.FileInputStream


import org.apache.spark.sql.{Row, SparkSession, Column}
import org.apache.spark.sql.expressions.{MutableAggregationBuffer, UserDefinedAggregateFunction}
import org.apache.spark.sql.types._

import java.sql.Timestamp

import org.apache.spark.sql.types._
import org.apache.spark.sql.catalyst.expressions.aggregate.{AggregateExpression, Complete}
import org.apache.spark.sql.execution.aggregate.ScalaUDAF

object ComputeExactFrequencies {
  def main(args: Array[String]) {

    val prop = new Properties()
    prop.load(new FileInputStream("app.properties"))
    val clusterId = prop.getProperty("cluster.id")
    val clusterName = prop.getProperty("cluster.name")
   
    /* Sliding window parameters */
    val windowLength = prop.getProperty("test.windowlength.seconds")
    val slideInterval = Minutes(prop.getProperty("test.windowslide.seconds").toInt)

    import org.apache.spark.sql._    
    import org.apache.spark.sql.Dataset
    import org.apache.spark.sql.streaming.StreamingQuery
    import org.apache.spark.sql.types._
    import org.apache.spark.sql.functions._
    import org.apache.spark.sql.functions.lit

    val spark = SparkSession
        .builder()
        .appName("ComputeExactFrequencies")
        .getOrCreate()
    
    import spark.implicits._

    val records = spark
	.readStream
	.format("text")
        .load("./files")
	.as[String]
	.flatMap(_.split(" "))
	.withColumn("timestamp", current_timestamp())
	
    /* Windowed computation: count all elements and sum them up */
    val windowedRecords = records
//	.withWatermark("timestamp", "10 minutes")
	.groupBy(window($"timestamp", windowLength+" seconds"),$"value")
	.count()
		
    /* Show schema after computation */
    windowedRecords.printSchema()
   
    import org.apache.spark.sql.streaming.{OutputMode, Trigger, ProcessingTime}

    import java.io._
    import org.apache.commons.io.FileUtils
    import java.util.logging.Logger
    val LOG = Logger.getLogger("ComputeFrequencies")

    /* Produce to file */
    val toFile = windowedRecords
	.writeStream
	.outputMode("complete")
	.foreach(new ForeachWriter[Row] {
	    var fileWriter: FileWriter = _

	    override def open(partitionId: Long, version: Long): Boolean = {
	      FileUtils.forceMkdir(new File(s"results/${partitionId}"))
	      fileWriter = new FileWriter(new File(s"results/${partitionId}/temp"))
	      true 
	    }

	    override def process(row: Row): Unit = {
//		fileWriter.append(row.toSeq.mkString(","))
		println(row.toSeq.mkString(","))

//	       val df = Seq(row.getString(0)).toDF
		
//		LOG.info(row.toSeq.mkString(","))
	
//	       df.write.format("text").mode("complete").save("results")
//		df.write.text("./results")
	    } 
	
	    override def close(errorOrNull: Throwable): Unit = {
		fileWriter.close()
	    }      
	  })
	.start()

//	.format("text")
//	.start()
//	.text("./results")
//	.writeStream
//	.outputMode("append")
//	.format("json")
//	.option("path", "./freqs")
//	.option("checkpointLocation", "checkpoint")
//	.foreach(writerForText)
//	.start()

    /* Print on console */
//    val toConsole = windowedRecords	
//	.orderBy($"count".desc)
//        .write
//        .format("text")
//        .option("truncate","false")
//        .outputMode("complete")
//        .trigger(ProcessingTime(60.seconds))
//        .start()

    toFile.awaitTermination(10000)
//    toConsole.awaitTermination(10000)

  }
}
